{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9m1KfKAE3ql"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "emotion_data = pd.read_csv(\"ratings_train.txt\", encoding='utf-8', sep='\\t')\n",
        "emotion_data_test = pd.read_csv(\"ratings_test.txt\", encoding='utf-8', sep='\\t')\n",
        "emotion_data = emotion_data.dropna()\n",
        "emotion_data_test = emotion_data_test.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "pW1w5aBSuvzA",
        "outputId": "7f1149a1-d37e-4f9e-b18b-a07ff2009aaa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>document</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "id          0\n",
              "document    0\n",
              "label       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emotion_data_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ml2Mq44J03o",
        "outputId": "cf00d196-0afc-41c7-e094-70cce0616210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7UV29sHW9lW",
        "outputId": "85e818db-7305-4778-8862-a72a01bfd514"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bHZqmVzFd_3",
        "outputId": "2b66de86-824c-4923-92f5-48e590e89792"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[['굳다', 'ㅋ'], 1],\n",
              " [[], 0],\n",
              " [['뭐', '야', '평점', '나쁘다', '않다', '점', '짜다', '리', '더', '더욱', '아니다'], 0],\n",
              " [['지루하다', '않다', '완전', '막장', '임', '돈', '주다', '보기', '에는'], 0],\n",
              " [['만', '아니다', '별', '다섯', '개', '주다', '왜', '로', '나오다', '제', '심기', '불편하다'], 0],\n",
              " [['음악', '주가', '되다', '최고', '음악', '영화'], 1],\n",
              " [['진정하다', '쓰레기'], 0],\n",
              " [['마치',\n",
              "   '미국',\n",
              "   '애니',\n",
              "   '에서',\n",
              "   '튀어나오다',\n",
              "   '창의력',\n",
              "   '없다',\n",
              "   '로봇',\n",
              "   '디자인',\n",
              "   '부터가',\n",
              "   '고개',\n",
              "   '젖다'],\n",
              "  0],\n",
              " [['갈수록',\n",
              "   '개판',\n",
              "   '되다',\n",
              "   '중국영화',\n",
              "   '유치하다',\n",
              "   '내용',\n",
              "   '없다',\n",
              "   '폼',\n",
              "   '잡다',\n",
              "   '끝나다',\n",
              "   '말',\n",
              "   '안되다',\n",
              "   '무기',\n",
              "   '유치하다',\n",
              "   '남무',\n",
              "   '아',\n",
              "   '그리다',\n",
              "   '동사서독',\n",
              "   '같다',\n",
              "   '영화',\n",
              "   '이건',\n",
              "   '류',\n",
              "   '아',\n",
              "   '류작',\n",
              "   '이다'],\n",
              "  0],\n",
              " [['이별', '아픔', '뒤', '찾아오다', '새롭다', '인연', '기쁨', '모든', '사람', '그렇다', '않다'], 1]]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
        "data_train = []\n",
        "data_test = []\n",
        "for sentence, label in zip(emotion_data['document'], emotion_data['label']):\n",
        "    data = []\n",
        "    sentence = [word for word in okt.morphs(re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', sentence), stem=True) if not word in stopwords]\n",
        "    data.append(sentence)\n",
        "    data.append(label)\n",
        "    data_train.append(data)\n",
        "\n",
        "for sentence, label in zip(emotion_data_test['document'], emotion_data_test['label']):\n",
        "    data = []\n",
        "    sentence = [word for word in okt.morphs(re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', sentence), stem=True) if not word in stopwords]\n",
        "    data.append(sentence)\n",
        "    data.append(label)\n",
        "    data_test.append(data)\n",
        "\n",
        "data_test[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVTwlF2XJv90"
      },
      "outputs": [],
      "source": [
        "# data_train, data_test 구조([[토큰리스트], 라벨])에서 토큰 리스트와 라벨 분리\n",
        "# 주의: data_train[i][0][0] 이 실제 토큰 리스트임\n",
        "import numpy as np\n",
        "X_train_tokens = [item[0] for item in data_train]\n",
        "y_train = np.array([item[1] for item in data_train])\n",
        "X_test_tokens = [item[0] for item in data_test]\n",
        "y_test = np.array([item[1] for item in data_test])\n",
        "\n",
        "# Tokenizer 생성 및 훈련 데이터 기준 단어 집합 생성\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrQOd0u2vUDk",
        "outputId": "8702f877-bf75-4c02-9c42-5c4d6655a872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "어휘 사전 크기: 43753\n"
          ]
        }
      ],
      "source": [
        "# 단어 집합 크기 확인 (OOV 고려 +1)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"\\n어휘 사전 크기: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0OwStQNvV2H"
      },
      "outputs": [],
      "source": [
        "# 텍스트를 정수 시퀀스로 변환\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train_tokens)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ2_Fcs0vaLe",
        "outputId": "24796bdf-2366-4072-bbc8-a862307a0873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "리뷰 최대 길이 계산 중...\n",
            "리뷰의 최대 길이 : 72\n"
          ]
        }
      ],
      "source": [
        "# 패딩을 위한 최대 길이 결정 (훈련 데이터 기준)\n",
        "print(\"\\n리뷰 최대 길이 계산 중...\")\n",
        "max_len = max(len(l) for l in X_train_sequences)\n",
        "print(f'리뷰의 최대 길이 : {max_len}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNGtIS9dvf0j",
        "outputId": "35def694-4963-4ff9-874f-28bda62acc60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "패딩 적용 중...\n",
            "패딩 완료.\n",
            "패딩 후 훈련 데이터 형태 (X): (149995, 72)\n",
            "훈련 라벨 데이터 형태 (y): (149995,)\n",
            "패딩 후 테스트 데이터 형태 (X): (49997, 72)\n",
            "테스트 라벨 데이터 형태 (y): (49997,)\n"
          ]
        }
      ],
      "source": [
        "# 패딩 적용\n",
        "print(\"패딩 적용 중...\")\n",
        "X_train_pad = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')\n",
        "print(\"패딩 완료.\")\n",
        "\n",
        "print(f\"패딩 후 훈련 데이터 형태 (X): {X_train_pad.shape}\")\n",
        "print(f\"훈련 라벨 데이터 형태 (y): {y_train.shape}\")\n",
        "print(f\"패딩 후 테스트 데이터 형태 (X): {X_test_pad.shape}\")\n",
        "print(f\"테스트 라벨 데이터 형태 (y): {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZX66VQXwQKV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt # 시각화 추가 (선택 사항)\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split # 검증 데이터 분리용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC3fPrl1vtso",
        "outputId": "372568a5-dd31-4150-a4b3-e278e2bd501b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "최종 훈련 데이터: (119996, 72), (119996,)\n",
            "검증 데이터: (29999, 72), (29999,)\n",
            "테스트 데이터: (49997, 72), (49997,)\n"
          ]
        }
      ],
      "source": [
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_pad, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"\\n최종 훈련 데이터: {X_train_final.shape}, {y_train_final.shape}\")\n",
        "print(f\"검증 데이터: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"테스트 데이터: {X_test_pad.shape}, {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYUQa_z7wG79",
        "outputId": "8afbbdfd-7f9c-4826-8025-e4efdb12d924"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 모델 학습 시작 ---\n",
            "Epoch 1/20\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.7768 - loss: 0.4508\n",
            "Epoch 1: val_accuracy improved from -inf to 0.84149, saving model to best_nsmc_lstm_model_v2.keras\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1084s\u001b[0m 287ms/step - accuracy: 0.7768 - loss: 0.4507 - val_accuracy: 0.8415 - val_loss: 0.3549\n",
            "Epoch 2/20\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.8739 - loss: 0.2987\n",
            "Epoch 2: val_accuracy improved from 0.84149 to 0.84809, saving model to best_nsmc_lstm_model_v2.keras\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1106s\u001b[0m 288ms/step - accuracy: 0.8739 - loss: 0.2987 - val_accuracy: 0.8481 - val_loss: 0.3473\n",
            "Epoch 3/20\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.8960 - loss: 0.2525\n",
            "Epoch 3: val_accuracy improved from 0.84809 to 0.84869, saving model to best_nsmc_lstm_model_v2.keras\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1057s\u001b[0m 282ms/step - accuracy: 0.8960 - loss: 0.2525 - val_accuracy: 0.8487 - val_loss: 0.3584\n",
            "Epoch 4/20\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9110 - loss: 0.2180\n",
            "Epoch 4: val_accuracy did not improve from 0.84869\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1056s\u001b[0m 282ms/step - accuracy: 0.9110 - loss: 0.2180 - val_accuracy: 0.8437 - val_loss: 0.3727\n",
            "Epoch 5/20\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.9247 - loss: 0.1873\n",
            "Epoch 5: val_accuracy did not improve from 0.84869\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1059s\u001b[0m 282ms/step - accuracy: 0.9247 - loss: 0.1874 - val_accuracy: 0.8450 - val_loss: 0.4029\n",
            "Epoch 6/20\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "# --- 5. LSTM 모델 구축, 컴파일, 학습, 평가, 예측 (이 부분은 이전과 거의 동일) ---\n",
        "\n",
        "# 모델 파라미터\n",
        "embedding_dim = 100\n",
        "lstm_units = 128\n",
        "\n",
        "# 모델 설계\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Bidirectional(LSTM(units=lstm_units)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 컴파일\n",
        "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# 콜백 설정\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_nsmc_lstm_model_v2.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True) # 저장 파일명 변경\n",
        "\n",
        "# 모델 학습\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "print(\"\\n--- 모델 학습 시작 ---\")\n",
        "history = model.fit(X_train_final, y_train_final,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[es, mc],\n",
        "                    verbose=1)\n",
        "\n",
        "# 모델 평가\n",
        "try:\n",
        "    loaded_model = load_model('best_nsmc_lstm_model_v2.keras') # 변경된 파일명으로 로드\n",
        "    print(\"\\n--- 베스트 모델 로드 후 테스트 데이터 평가 ---\")\n",
        "    loss, accuracy = loaded_model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "except Exception as e:\n",
        "    print(f\"\\n모델 로드 실패: {e}. 마지막 학습 모델로 평가합니다.\")\n",
        "    loaded_model = model\n",
        "    print(\"\\n--- 마지막 학습 모델 테스트 데이터 평가 ---\")\n",
        "    loss, accuracy = loaded_model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "\n",
        "print(f\"테스트 손실: {loss:.4f}\")\n",
        "print(f\"테스트 정확도: {accuracy:.4f}\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# 예측 함수 정의 (내부 전처리 로직은 사용자 원본과 동일하게 유지)\n",
        "def predict_sentiment(text, tokenizer, model, max_len):\n",
        "    # 1. 텍스트 정제 및 토큰화 (사용자 원본 방식 유지)\n",
        "    processed_text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', str(text))\n",
        "    tokenized_text = [word for word in okt.morphs(processed_text, stem=True) if not word in stopwords and len(word) > 1]\n",
        "\n",
        "    if not tokenized_text:\n",
        "        return \"분석할 유효한 토큰이 없습니다.\"\n",
        "\n",
        "    # 2. 정수 인코딩\n",
        "    try:\n",
        "        sequence = tokenizer.texts_to_sequences([tokenized_text])\n",
        "    except Exception as e:\n",
        "        return f\"정수 인코딩 오류: {e}\"\n",
        "\n",
        "    # 3. 패딩\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "\n",
        "    # 4. 예측\n",
        "    try:\n",
        "        if padded_sequence.shape != (1, max_len):\n",
        "             padded_sequence = padded_sequence.reshape(1, max_len)\n",
        "        prediction = model.predict(padded_sequence, verbose=0)\n",
        "    except Exception as e:\n",
        "        return f\"모델 예측 오류: {e}, 입력 형태: {padded_sequence.shape}\"\n",
        "\n",
        "    # 5. 결과 해석\n",
        "    score = float(prediction[0][0])\n",
        "    if score > 0.5:\n",
        "        return f\"긍정\"\n",
        "    else:\n",
        "        return f\"부정\"\n",
        "\n",
        "\n",
        "# 새로운 텍스트 예측 테스트\n",
        "print(\"\\n--- 새로운 텍스트 예측 ---\")\n",
        "test_sentence_1 = \"이 영화 진짜 재밌음 ㅋㅋ 강추!!\"\n",
        "result_1 = predict_sentiment(test_sentence_1, tokenizer, loaded_model, max_len)\n",
        "print(f\"'{test_sentence_1}' -> {result_1}\")\n",
        "\n",
        "test_sentence_2 = \"별로였어요. 돈 아까움.\"\n",
        "result_2 = predict_sentiment(test_sentence_2, tokenizer, loaded_model, max_len)\n",
        "print(f\"'{test_sentence_2}' -> {result_2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcvtEZZUzOFu"
      },
      "outputs": [],
      "source": [
        "# 흥부를 묘사하는 문장 분석\n",
        "print(\"\\n--- 흥부 텍스트 예측 ---\")\n",
        "Heungbu = [\"흥부는 고운 마음씨로 사람들을 도왔다.\",\n",
        "           \"흥부는 마음씨 착하고 효행이 지극하며 동기간의 우애가 극진한데\",\n",
        "           \"흥부는 충실, 온 후, 인자하였으니, 형의 하는 짓을 탄식하고 때로는 간할 마음을 가져보았으나 말해 보아야 쓸데없으므로 말없이 주면 먹고 시키는 일이나 공손히 하였다.\",\n",
        "           \"흥부의 어진 마음은 조금도 변함이 없었다.\",\n",
        "           \"흥부는 울며 사정하였다.\",\n",
        "           \"흥부는 과연 어진 사람이다. 공 있는 자에게 보은함은 군자의 도리이니, 그 은혜를 어찌 아니 갚으랴？\",\n",
        "           \"흥부의 구조를 받아 살아서 돌아왔습니다.\",\n",
        "           \"흥부의 가난을 면케 해주신다면 그로써 소신은 그 은공의 만분의 일이라도 갚을까 합니다.\",\n",
        "           \"흥부가 거절하자 이번은 화초장이나 달라고 한다.\",\n",
        "           \"흥부는 안방을 치우고 형님 내외를 거처케 한 다음 의식을 후히 내어 대접하며 위로하고, 한편으로 좋은 터를 잡아 수만금을 아낌없이 들여 집을 짓되 제 집과 같게 하고 세간이며 의복 음식을 똑같게 하여 그 형을 살게 하여 주었다.\",\n",
        "           \"밥 좀 많이 붙은 주걱으로요. 그 밥 갖다가 아이들 구경이나 시키겠소.\",\n",
        "           \"흥부는 얼른 받아 제비 새끼의 상한 다리를 곱게 감아 매어 찬 이슬에 얹어 두었다.\"\n",
        "\n",
        "]\n",
        "Heungbu_label = []\n",
        "\n",
        "for test_sentence_1 in Heungbu:\n",
        "  result_1 = predict_sentiment(test_sentence_1, tokenizer, loaded_model, max_len)\n",
        "  if result_1 == \"긍정\":\n",
        "    Heungbu_label.append(1)\n",
        "  else:\n",
        "    Heungbu_label.append(0)\n",
        "\n",
        "  print(f\"'{test_sentence_1}' -> {result_1}\")\n",
        "\n",
        "\n",
        "# 놀부를 묘사하는 문장 분석\n",
        "print(\"\\n--- 놀부 텍스트 예측 ---\")\n",
        "Nolbu = [\"놀부는 부모에게는 불효이고 동기간에 우애가 조금도 없으니, 그 마음 쓰는 것이 괴상하였다.\",\n",
        "         \"놀부는 애초부터 오장에 칠 부였다. 말하자면 심술보가 하나 더 있어 심술보가 한 번만 뒤집히면 심사를 야단스럽게도 피웠다.\",\n",
        "         \"이놈의 심사가 이렇듯 모과나무같이 뒤틀리고 동풍 안개 속에 수숫잎, 같이 꼬여 그 흉악함을 헤아릴 수 없었다.\",\n",
        "         \"놀부의 악한 마음은 부모가 물려준 많은 재산을 독차지하고 아우 흥부를 구박하나\",\n",
        "        \"놀부는 화를 더럭 내어 벼락같은 소리로 하인 마당쇠를 부르는 것이었다.\",\n",
        "        \"이놈이 도둑질했나？ 내가 가서 욱대기면 반 재산을 뺏어낼 것이다.\",\n",
        "        \"제비 새끼 잡아 두 발목을 지끈등 분지르고는\",\n",
        "        \"원수 같은 놀부놈아, 다음 해 춘삼월에 다시 와서 원수를 갚을 것이니 잘 있거라. 지지위 지지.\",\n",
        "        \"놀부놈은 기겁을 하여 돈 오천 냥을내주고 겨우 그들을 보내고 나니 열이 치받쳤다.\",\n",
        "         \"이놈 놀부야, 돈이 중하냐 목숨이 중하냐？\",\n",
        "         \"이놈 놀부야, 네가 세상에 태어나 부모께 불효요, 형제에게 불목하고 친척과 불화하니 죄악이 네 털을 빼어 세어도 당치 못할 것이다.\",\n",
        "         \"놀부가 기가 막혀 발을 동동 구르며 탄식하였다.\",\n",
        "         ]\n",
        "\n",
        "Nolbu_label = []\n",
        "\n",
        "for test_sentence_2 in Nolbu:\n",
        "  test_sentence_2 = test_sentence_2\n",
        "  result_2 = predict_sentiment(test_sentence_2, tokenizer, loaded_model, max_len)\n",
        "  print(f\"'{test_sentence_2}' -> {result_2}\")\n",
        "\n",
        "  if result_2 == \"긍정\":\n",
        "    Nolbu_label.append(1)\n",
        "  else:\n",
        "    Nolbu_label.append(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6YB7gW41IrR"
      },
      "outputs": [],
      "source": [
        "heungbu_pos_count = sum(label == 1 for label in Heungbu_label)\n",
        "heungbu_neg_count = sum(label == 0 for label in Heungbu_label)\n",
        "nolbu_pos_count = sum(label == 1 for label in Heungbu_label)\n",
        "nolbu_neg_count = sum(label == 0 for label in Heungbu_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uyK-fCV1tdG"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}